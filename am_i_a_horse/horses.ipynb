{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network for Horse-Human Binary Classification\n",
    "\n",
    "This Jupyter Notebook builds a binary classifier in PyTorch using the \"$\\textit{horse or human}$\" dataset for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary packages. Three non-standard packages used are $\\texttt{Pytorch}$, $\\texttt{NumPy}$ and $\\texttt{OpenCV}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import glob\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "from typing import Union, NamedTuple, List, Set, Dict, Tuple, Optional\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and validation images are available for download from https://laurencemoroney.com/datasets.html. Uncomment the lines below if the dataset has not been unzipped yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import zipfile\n",
    "\n",
    "# local_zip = 'horse-or-human.zip'\n",
    "# zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "# zip_ref.extractall('horse-or-human')\n",
    "# zip_ref.close()\n",
    "# local_zip = 'validation-horse-or-human.zip'\n",
    "# zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "# zip_ref.extractall('validation-horse-or-human')\n",
    "# zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset\n",
    "We have to build our own Dataset class, bacuase we can't use an easy built-in dataset (such as MNIST etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_name: str, transform=None):\n",
    "        self.imgs_path = folder_name\n",
    "        self.transform = transform\n",
    "        file_list = glob.glob(self.imgs_path + \"*\")\n",
    "        self.data = []\n",
    "        for class_path in file_list:\n",
    "            class_name = class_path.split(\"/\")[-1]\n",
    "            for img_path in glob.glob(class_path + \"/*.png\"):\n",
    "                self.data.append([img_path, class_name])\n",
    "        self.class_map = {\"horses\" : 0, \"humans\": 1}\n",
    "        self.img_dim = (128, 128)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_name = self.data[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, self.img_dim)\n",
    "        class_id = self.class_map[class_name]\n",
    "\n",
    "        class_id = torch.tensor(class_id)\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img)*255\n",
    "        else:\n",
    "            img_tensor = torch.from_numpy(img).permute(2,0,1)\n",
    "        img_tensor = img_tensor.to(torch.float)\n",
    "#         cv2.imshow('show',np.array(img_tensor.permute(1, 2, 0).cpu().detach().numpy(), dtype=np.uint8()))\n",
    "#         cv2.waitKey(1000)\n",
    "        return img_tensor, class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "Data augmentation is a must when the dataset is as very small as this. We apply\n",
    "- random horizontal flip\n",
    "- random rotation\n",
    "- random crop\n",
    "- random hsv and contrast adjustment\n",
    "- randomly sets to grayscale\n",
    "\n",
    "This should provide more variety in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1027\n",
      "Test dataset size: 256\n"
     ]
    }
   ],
   "source": [
    "transformation = transforms.Compose([\n",
    "    transforms.ToPILImage(mode=None),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply(torch.nn.ModuleList([\n",
    "        transforms.Grayscale(num_output_channels=3)\n",
    "    ]), p=0.1),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomResizedCrop(size=(128, 128), scale=(0.5, 1.2), ratio=(0.9, 1.1)),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset_train = CustomDataset(\"horse-or-human/\", transform=transformation)\n",
    "print('Train dataset size: ' + str(len(dataset_train)))\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_test = CustomDataset(\"validation-horse-or-human/\")\n",
    "print('Test dataset size: ' + str(len(dataset_test)))\n",
    "val_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageShape(NamedTuple):\n",
    "    height: int\n",
    "    width: int\n",
    "    channels: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "The network architecture is as follows:\n",
    "\n",
    "Image (128,128,3) $\\rightarrow$ Conv (32) $\\rightarrow$ Pool (2,2) $\\rightarrow$ Conv (64) $\\rightarrow$ Pool (2,2) $\\rightarrow$ Conv (64) $\\rightarrow$ Pool (2,2) $\\rightarrow$ FullyConnected (128) $\\rightarrow$ FullyConnected (2).\n",
    "\n",
    "The activation function is leaky rectified linear unit (leaky ReLU), there is batch normalisation to appease internal covariance shift, there is dropout for better generalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseNN(nn.Module):\n",
    "    def __init__(self, height: int, width: int, channels: int, class_count: int):\n",
    "        super(HorseNN, self).__init__()\n",
    "        self.input_shape = ImageShape(height=height, width=width, channels=channels)\n",
    "        self.class_count = class_count\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=self.input_shape.channels,out_channels=32,kernel_size=(5, 5),padding=(2, 2))\n",
    "        self.initialise_layer(self.conv1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=(5, 5),padding=(2, 2))\n",
    "        self.initialise_layer(self.conv2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(5, 5),padding=(2, 2))\n",
    "        self.initialise_layer(self.conv2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16*16*64, 128)\n",
    "        self.initialise_layer(self.fc1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.initialise_layer(self.fc2)\n",
    "        \n",
    "        self.convolution = nn.Sequential(\n",
    "            self.conv1,\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.02, True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            self.conv2,\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.02, True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            self.conv3,\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.02, True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        )\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Dropout(0.6),\n",
    "            self.fc1,\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.02, True),\n",
    "            nn.Dropout(0.6),\n",
    "            self.fc2\n",
    "        )\n",
    "        \n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolution(images)\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        x = self.fully_connected(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def initialise_layer(layer):\n",
    "        if hasattr(layer, \"bias\"):\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        if hasattr(layer, \"weight\"):\n",
    "            nn.init.kaiming_normal_(layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(\n",
    "    labels: Union[torch.Tensor, np.ndarray], preds: Union[torch.Tensor, np.ndarray]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels: ``(batch_size, class_count)`` tensor or array containing example labels\n",
    "        preds: ``(batch_size, class_count)`` tensor or array containing model prediction\n",
    "    \"\"\"\n",
    "    assert len(labels) == len(preds)\n",
    "    return float((labels == preds).sum()) / len(labels)\n",
    "\n",
    "\n",
    "def get_summary_writer_log_dir(batch_size, learning_rate) -> str:\n",
    "    return \"\"\n",
    "    \"\"\"Get a unique directory that hasn't been logged to before for use with a TB\n",
    "    SummaryWriter.\n",
    "    Args:\n",
    "        args: CLI Arguments\n",
    "    Returns:\n",
    "        Subdirectory of log_dir with unique subdirectory name to prevent multiple runs\n",
    "        from getting logged to the same TB log directory (which you can't easily\n",
    "        untangle in TB).\n",
    "    \"\"\"\n",
    "    tb_log_dir_prefix = f'CNN_bs={batch_size}_lr={learning_rate}_run_'\n",
    "    i = 0\n",
    "    while i < 1000:\n",
    "        tb_log_dir = args.log_dir / (tb_log_dir_prefix + str(i))\n",
    "        if not tb_log_dir.exists():\n",
    "            return str(tb_log_dir)\n",
    "        i += 1\n",
    "    return str(tb_log_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if gpu is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kris/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Class Trainer is responsible for the training of the model, fetch data, forward pass, optimisation, back-progpagation etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        criterion: nn.Module,\n",
    "        optimizer: Optimizer,\n",
    "        summary_writer: SummaryWriter,\n",
    "        device: torch.device,\n",
    "        save_models: List[Tuple[nn.Module, float]]\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.summary_writer = summary_writer\n",
    "        self.step = 0\n",
    "        self.save_models = save_models\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        epochs: int,\n",
    "        val_frequency: int,\n",
    "        print_frequency: int = 20,\n",
    "        log_frequency: int = 5,\n",
    "        start_epoch: int = 0\n",
    "    ):\n",
    "        self.model.train()\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            self.model.train()\n",
    "            data_load_start_time = time.time()\n",
    "            for batch, labels in self.train_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                data_load_end_time = time.time()\n",
    "\n",
    "\n",
    "                logits = self.model.forward(batch)\n",
    "\n",
    "                loss = self.criterion(logits, labels)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    preds = logits.argmax(-1)\n",
    "                    accuracy = compute_accuracy(labels, preds)\n",
    "\n",
    "                data_load_time = data_load_end_time - data_load_start_time\n",
    "                step_time = time.time() - data_load_end_time\n",
    "                if ((self.step + 1) % log_frequency) == 0:\n",
    "                    self.log_metrics(epoch, accuracy, loss, data_load_time, step_time)\n",
    "                if ((self.step + 1) % print_frequency) == 0:\n",
    "                    self.print_metrics(epoch, accuracy, loss, data_load_time, step_time)\n",
    "\n",
    "                self.step += 1\n",
    "                data_load_start_time = time.time()\n",
    "\n",
    "            self.summary_writer.add_scalar(\"epoch\", epoch, self.step)\n",
    "            if ((epoch + 1) % val_frequency) == 0:\n",
    "                self.validate()\n",
    "                # self.validate() will put the model in validation mode,\n",
    "                # so we have to switch back to train mode afterwards\n",
    "                self.model.train()\n",
    "\n",
    "    def print_metrics(self, epoch, accuracy, loss, data_load_time, step_time):\n",
    "        epoch_step = self.step % len(self.train_loader)\n",
    "        print(\n",
    "                f\"epoch: [{epoch}], \"\n",
    "                f\"step: [{epoch_step}/{len(self.train_loader)}], \"\n",
    "                f\"batch loss: {loss:.5f}, \"\n",
    "                f\"batch accuracy: {accuracy * 100:2.2f}, \"\n",
    "                f\"data load time: \"\n",
    "                f\"{data_load_time:.5f}, \"\n",
    "                f\"step time: {step_time:.5f}\"\n",
    "        )\n",
    "\n",
    "    def log_metrics(self, epoch, accuracy, loss, data_load_time, step_time):\n",
    "        self.summary_writer.add_scalar(\"epoch\", epoch, self.step)\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"accuracy\",\n",
    "                {\"train\": accuracy},\n",
    "                self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"loss\",\n",
    "                {\"train\": float(loss.item())},\n",
    "                self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalar(\n",
    "                \"time/data\", data_load_time, self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalar(\n",
    "                \"time/data\", step_time, self.step\n",
    "        )\n",
    "\n",
    "    def validate(self):\n",
    "        results = {\"preds\": [], \"labels\": []}\n",
    "        total_loss = 0\n",
    "        self.model.eval()\n",
    "\n",
    "        # No need to track gradients for validation, we're not optimizing.\n",
    "        with torch.no_grad():\n",
    "            for batch, labels in self.val_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                logits = self.model(batch)\n",
    "                loss = self.criterion(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "                preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "                results[\"preds\"].extend(list(preds))\n",
    "                results[\"labels\"].extend(list(labels.cpu().numpy()))\n",
    "\n",
    "        accuracy = compute_accuracy(\n",
    "            np.array(results[\"labels\"]), np.array(results[\"preds\"])\n",
    "        )\n",
    "        average_loss = total_loss / len(self.val_loader)\n",
    "        self.save_models.append((deepcopy(self.model.state_dict()), accuracy))\n",
    "\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"accuracy\",\n",
    "                {\"test\": accuracy},\n",
    "                self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"loss\",\n",
    "                {\"test\": average_loss},\n",
    "                self.step\n",
    "        )\n",
    "        print(f\"validation loss: {average_loss:.5f}, accuracy: {accuracy * 100:2.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to \n",
      "epoch: [0], step: [1/17], batch loss: 0.66488, batch accuracy: 68.75, data load time: 0.86164, step time: 2.31496\n",
      "epoch: [0], step: [3/17], batch loss: 0.63468, batch accuracy: 73.44, data load time: 0.79027, step time: 2.37861\n",
      "epoch: [0], step: [5/17], batch loss: 0.77927, batch accuracy: 67.19, data load time: 0.81880, step time: 2.44448\n",
      "epoch: [0], step: [7/17], batch loss: 0.43584, batch accuracy: 81.25, data load time: 0.84438, step time: 2.44112\n",
      "epoch: [0], step: [9/17], batch loss: 0.82480, batch accuracy: 64.06, data load time: 0.91859, step time: 2.36952\n",
      "epoch: [0], step: [11/17], batch loss: 0.52750, batch accuracy: 81.25, data load time: 0.95038, step time: 2.93334\n",
      "epoch: [0], step: [13/17], batch loss: 0.64953, batch accuracy: 70.31, data load time: 0.89654, step time: 2.59211\n",
      "epoch: [0], step: [15/17], batch loss: 0.51164, batch accuracy: 75.00, data load time: 0.99659, step time: 2.74845\n",
      "validation loss: 0.65131, accuracy: 89.84\n",
      "epoch: [1], step: [0/17], batch loss: 0.38757, batch accuracy: 82.81, data load time: 0.84194, step time: 2.51709\n",
      "epoch: [1], step: [2/17], batch loss: 0.40162, batch accuracy: 82.81, data load time: 0.82475, step time: 2.50596\n",
      "epoch: [1], step: [4/17], batch loss: 0.23762, batch accuracy: 85.94, data load time: 0.87783, step time: 2.44326\n",
      "epoch: [1], step: [6/17], batch loss: 0.35824, batch accuracy: 82.81, data load time: 0.84375, step time: 2.46488\n",
      "epoch: [1], step: [8/17], batch loss: 0.36092, batch accuracy: 82.81, data load time: 0.82019, step time: 2.40618\n",
      "epoch: [1], step: [10/17], batch loss: 0.56051, batch accuracy: 76.56, data load time: 0.83384, step time: 2.40676\n",
      "epoch: [1], step: [12/17], batch loss: 0.38733, batch accuracy: 81.25, data load time: 0.78550, step time: 2.36128\n",
      "epoch: [1], step: [14/17], batch loss: 0.28328, batch accuracy: 89.06, data load time: 0.82722, step time: 2.50800\n",
      "epoch: [1], step: [16/17], batch loss: 0.15757, batch accuracy: 100.00, data load time: 0.03741, step time: 0.12408\n",
      "validation loss: 0.98748, accuracy: 80.86\n",
      "epoch: [2], step: [1/17], batch loss: 0.37438, batch accuracy: 82.81, data load time: 0.86335, step time: 2.56592\n",
      "epoch: [2], step: [3/17], batch loss: 0.31978, batch accuracy: 85.94, data load time: 0.88145, step time: 2.37109\n",
      "epoch: [2], step: [5/17], batch loss: 0.56863, batch accuracy: 75.00, data load time: 0.76528, step time: 2.42003\n",
      "epoch: [2], step: [7/17], batch loss: 0.53002, batch accuracy: 76.56, data load time: 0.78932, step time: 2.37916\n",
      "epoch: [2], step: [9/17], batch loss: 0.19037, batch accuracy: 89.06, data load time: 1.63215, step time: 3.36573\n",
      "epoch: [2], step: [11/17], batch loss: 0.29994, batch accuracy: 85.94, data load time: 0.98943, step time: 2.65478\n",
      "epoch: [2], step: [13/17], batch loss: 0.60955, batch accuracy: 75.00, data load time: 1.13664, step time: 3.13380\n",
      "epoch: [2], step: [15/17], batch loss: 0.36477, batch accuracy: 79.69, data load time: 0.91754, step time: 2.76610\n",
      "validation loss: 0.36068, accuracy: 90.62\n",
      "epoch: [3], step: [0/17], batch loss: 0.29894, batch accuracy: 87.50, data load time: 1.00076, step time: 2.73232\n",
      "epoch: [3], step: [2/17], batch loss: 0.43051, batch accuracy: 81.25, data load time: 0.98054, step time: 2.71331\n",
      "epoch: [3], step: [4/17], batch loss: 0.50053, batch accuracy: 76.56, data load time: 0.99036, step time: 2.75252\n",
      "epoch: [3], step: [6/17], batch loss: 0.60042, batch accuracy: 71.88, data load time: 1.01762, step time: 2.80177\n",
      "epoch: [3], step: [8/17], batch loss: 0.33354, batch accuracy: 82.81, data load time: 0.98489, step time: 2.77891\n",
      "epoch: [3], step: [10/17], batch loss: 0.26225, batch accuracy: 87.50, data load time: 0.92677, step time: 2.69849\n",
      "epoch: [3], step: [12/17], batch loss: 0.46646, batch accuracy: 81.25, data load time: 0.92030, step time: 3.00424\n",
      "epoch: [3], step: [14/17], batch loss: 0.39030, batch accuracy: 82.81, data load time: 0.92511, step time: 2.72459\n",
      "epoch: [3], step: [16/17], batch loss: 0.97052, batch accuracy: 33.33, data load time: 0.04193, step time: 0.12789\n",
      "validation loss: 0.35772, accuracy: 85.55\n",
      "epoch: [4], step: [1/17], batch loss: 0.34696, batch accuracy: 87.50, data load time: 0.84080, step time: 2.75507\n",
      "epoch: [4], step: [3/17], batch loss: 0.42671, batch accuracy: 78.12, data load time: 0.91458, step time: 2.71282\n",
      "epoch: [4], step: [5/17], batch loss: 0.37091, batch accuracy: 89.06, data load time: 1.00057, step time: 2.67128\n",
      "epoch: [4], step: [7/17], batch loss: 0.35885, batch accuracy: 79.69, data load time: 0.95010, step time: 2.81236\n",
      "epoch: [4], step: [9/17], batch loss: 0.33873, batch accuracy: 82.81, data load time: 0.93738, step time: 2.70932\n",
      "epoch: [4], step: [11/17], batch loss: 0.29046, batch accuracy: 89.06, data load time: 1.15378, step time: 2.84843\n",
      "epoch: [4], step: [13/17], batch loss: 0.19523, batch accuracy: 93.75, data load time: 1.03314, step time: 2.74459\n",
      "epoch: [4], step: [15/17], batch loss: 0.21161, batch accuracy: 93.75, data load time: 0.97195, step time: 2.72570\n",
      "validation loss: 0.68936, accuracy: 81.25\n",
      "epoch: [5], step: [0/17], batch loss: 0.34991, batch accuracy: 87.50, data load time: 1.01699, step time: 2.68834\n",
      "epoch: [5], step: [2/17], batch loss: 0.23963, batch accuracy: 90.62, data load time: 0.95882, step time: 2.76348\n",
      "epoch: [5], step: [4/17], batch loss: 0.28567, batch accuracy: 90.62, data load time: 0.95024, step time: 3.20957\n",
      "epoch: [5], step: [6/17], batch loss: 0.23127, batch accuracy: 89.06, data load time: 0.92619, step time: 2.79910\n",
      "epoch: [5], step: [8/17], batch loss: 0.27140, batch accuracy: 89.06, data load time: 1.14441, step time: 3.44278\n",
      "epoch: [5], step: [10/17], batch loss: 0.28059, batch accuracy: 84.38, data load time: 0.90509, step time: 2.69892\n",
      "epoch: [5], step: [12/17], batch loss: 0.22500, batch accuracy: 89.06, data load time: 0.94703, step time: 2.73726\n",
      "epoch: [5], step: [14/17], batch loss: 0.31035, batch accuracy: 92.19, data load time: 0.90523, step time: 2.66068\n",
      "epoch: [5], step: [16/17], batch loss: 0.22485, batch accuracy: 100.00, data load time: 0.04758, step time: 0.14931\n",
      "validation loss: 0.78940, accuracy: 73.44\n",
      "epoch: [6], step: [1/17], batch loss: 0.20588, batch accuracy: 90.62, data load time: 0.99228, step time: 2.70343\n",
      "epoch: [6], step: [3/17], batch loss: 0.42994, batch accuracy: 84.38, data load time: 0.97638, step time: 2.73180\n",
      "epoch: [6], step: [5/17], batch loss: 0.36528, batch accuracy: 84.38, data load time: 0.92178, step time: 2.70041\n",
      "epoch: [6], step: [7/17], batch loss: 0.33818, batch accuracy: 87.50, data load time: 0.95684, step time: 2.82587\n",
      "epoch: [6], step: [9/17], batch loss: 0.20399, batch accuracy: 93.75, data load time: 0.89461, step time: 2.87287\n",
      "epoch: [6], step: [11/17], batch loss: 0.23521, batch accuracy: 89.06, data load time: 0.96308, step time: 2.74909\n",
      "epoch: [6], step: [13/17], batch loss: 0.32085, batch accuracy: 89.06, data load time: 0.96853, step time: 2.65950\n",
      "epoch: [6], step: [15/17], batch loss: 0.21958, batch accuracy: 93.75, data load time: 0.99426, step time: 2.81280\n",
      "validation loss: 4.18394, accuracy: 54.69\n",
      "epoch: [7], step: [0/17], batch loss: 0.36356, batch accuracy: 85.94, data load time: 0.97404, step time: 2.76161\n",
      "epoch: [7], step: [2/17], batch loss: 0.47023, batch accuracy: 81.25, data load time: 1.09736, step time: 2.68410\n",
      "epoch: [7], step: [4/17], batch loss: 0.43471, batch accuracy: 78.12, data load time: 0.90677, step time: 2.45529\n",
      "epoch: [7], step: [6/17], batch loss: 0.26540, batch accuracy: 89.06, data load time: 0.84531, step time: 2.41034\n",
      "epoch: [7], step: [8/17], batch loss: 0.32942, batch accuracy: 90.62, data load time: 0.90898, step time: 2.38579\n",
      "epoch: [7], step: [10/17], batch loss: 0.26964, batch accuracy: 85.94, data load time: 0.77921, step time: 2.91841\n",
      "epoch: [7], step: [12/17], batch loss: 0.23088, batch accuracy: 87.50, data load time: 0.99993, step time: 2.63503\n",
      "epoch: [7], step: [14/17], batch loss: 0.20607, batch accuracy: 90.62, data load time: 0.81880, step time: 2.74792\n",
      "epoch: [7], step: [16/17], batch loss: 0.44077, batch accuracy: 66.67, data load time: 0.04728, step time: 0.14640\n",
      "validation loss: 0.55968, accuracy: 83.59\n"
     ]
    }
   ],
   "source": [
    "model = HorseNN(128, 128, 3, 2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 5e-3\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 8\n",
    "save_models = []\n",
    "\n",
    "log_dir = get_summary_writer_log_dir(batch_size, learning_rate)\n",
    "print(f\"Writing logs to {log_dir}\")\n",
    "summary_writer = SummaryWriter(\n",
    "        str(log_dir),\n",
    "        flush_secs=5\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model, train_loader, val_loader, criterion, optimizer, summary_writer, DEVICE, save_models\n",
    ")\n",
    "\n",
    "trainer.train(\n",
    "    num_epochs,\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    ")\n",
    "\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the model for classifier\n",
    "The model is saved after each epoch with the validation accuracy. We choose the lowest accuracy model as our classifier and save it in a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.36068, accuracy: 90.62\n"
     ]
    }
   ],
   "source": [
    "trainer.save_models.sort(key=lambda x: x[1], reverse=True)\n",
    "trainer.model = HorseNN(128, 128, 3, 2)\n",
    "trainer.model.load_state_dict(trainer.save_models[0][0])\n",
    "trainer.validate() # can check which model is actually chosen\n",
    "torch.save(trainer.save_models[0][0], 'my_horse_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if you or your friends are horses :)\n",
    "Or if your horse is a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9964, 0.0036]], grad_fn=<SoftmaxBackward>)\n",
      "It is 0.9964 percent you're a horse.\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "classes = { 0: 'horse', 1: \"human\"}\n",
    "\n",
    "mymodel = HorseNN(128, 128, 3, 2)\n",
    "mymodel.load_state_dict(torch.load('my_horse_model.pth'))\n",
    "mymodel.eval()\n",
    "\n",
    "img = cv2.imread('white_horse.jpg')\n",
    "if img.shape[0] < img.shape[1]:\n",
    "    delta = img.shape[1] - img.shape[0]\n",
    "    img = np.pad(img, ((delta // 2, delta // 2), (0, 0), (0, 0)))\n",
    "else:\n",
    "    delta = img.shape[0] - img.shape[1]\n",
    "    img = np.pad(img, ((0, 0), (delta // 2, delta // 2), (0, 0)))\n",
    "img = cv2.resize(img, (128, 128))\n",
    "# cv2.imshow('window', img)\n",
    "# cv2.waitKey(0)\n",
    "img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
    "img_tensor = img_tensor.to(torch.float)\n",
    "\n",
    "result = mymodel.forward(batch)\n",
    "\n",
    "result = softmax(result)\n",
    "print(result)\n",
    "result = result[0]\n",
    "\n",
    "print(\"It is %.4f percent you're a horse.\" % float(result[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
